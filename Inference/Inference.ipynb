{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dee5cf4",
   "metadata": {},
   "source": [
    "#Demo of AsymFormer: Evaluation and Inference Speed Test\n",
    "Note: The entire testing process must be conducted on an Ubuntu operating system with support for pycuda and TensorRT. We recommend using Pytorch>=2.0, Cuda>=12.0 to run the speed test. The inference speed reported in the paper was tested on RTX 3090 platform, with Ubuntu 20.04, Cuda 12.0, Pytorch 2.0.1, opencv-python 4.5.5.64, TensorRT 8.6.0 and pycuda 2022.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8b864",
   "metadata": {},
   "source": [
    "#Step.1: Import necessary packages and define the data transform functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c97a5-9f9d-45df-bfde-4dc05cf08c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Import necessary packages\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "import cv2\n",
    "import NYUv2_dataloader as Data\n",
    "from utils.utils import intersectionAndUnion, AverageMeter, accuracy, macc\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "\"\"\"Set the image size in inference\"\"\"\n",
    "image_w = 640\n",
    "image_h = 480\n",
    "\n",
    "\"\"\"Data Transform: Resize, ToTensor and Normalization\"\"\"\n",
    "# transform\n",
    "class scaleNorm(object):\n",
    "    def __call__(self, sample):\n",
    "        image, depth, label = sample['image'], sample['depth'], sample['label']\n",
    "\n",
    "        label = label.astype(np.int16)\n",
    "        # Bi-linear\n",
    "        image = cv2.resize(image, (image_w, image_h), cv2.INTER_LINEAR)\n",
    "        # Nearest-neighbor\n",
    "        depth = cv2.resize(depth, (image_w, image_h), cv2.INTER_NEAREST)\n",
    "        label = cv2.resize(label, (image_w, image_h), cv2.INTER_NEAREST)\n",
    "\n",
    "        return {'image': image, 'depth': depth, 'label': label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, depth, label = sample['image'], sample['depth'], sample['label']\n",
    "\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        depth = np.expand_dims(depth, 0)\n",
    "        return {'image': torch.from_numpy(image).float(),\n",
    "                'depth': torch.from_numpy(depth).float(),\n",
    "                'label': torch.from_numpy(label).float()}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, depth = sample['image'], sample['depth']\n",
    "        origin_image = image.clone()\n",
    "        origin_depth = depth.clone()\n",
    "        image = image / 255\n",
    "\n",
    "        image = torchvision.transforms.Normalize(mean=[0.4850042694973687, 0.41627756261047333, 0.3981809741523051],\n",
    "                                                 std=[0.26415541082494515, 0.2728415392982039, 0.2831175140191598])(\n",
    "            image)\n",
    "\n",
    "        depth = torchvision.transforms.Normalize(mean=[2.8424503515351494], std=[0.9932836506164299])(depth)\n",
    "        sample['origin_image'] = origin_image\n",
    "        sample['origin_depth'] = origin_depth\n",
    "        sample['image'] = image\n",
    "        sample['depth'] = depth\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243309b8",
   "metadata": {},
   "source": [
    "#Setp.2: Load the prprocessed TensorRT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ba23b-1358-4555-a284-eee64fad112b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"AsymFormer.engine\", \"rb\")                     # Open the TensorRT model. In this case, the model is put in same folder as this jupyter notebook.\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))   #  Sets up a TensorRT runtime engine with a warning-level logger\n",
    "engine = runtime.deserialize_cuda_engine(f.read())      # Load TensorRT inference engine from the '.engine' file.\n",
    "\n",
    "\"\"\"creates an execution context object that corresponds to the TensorRT engine. \n",
    "The execution context will be used later on to execute inference tasks on the engine, using the defined optimized model and settings.\"\"\"\n",
    "context = engine.create_execution_context()             \n",
    "\n",
    "\"\"\"Setup I/O bindings\"\"\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "allocations = []\n",
    "\n",
    "\"\"\"Allocate memory for the input and output\"\"\"\n",
    "for i in range(engine.num_bindings): \n",
    "    is_input = False\n",
    "    if engine.binding_is_input(i):\n",
    "        is_input = True\n",
    "    name = engine.get_binding_name(i)\n",
    "    dtype = engine.get_binding_dtype(i)\n",
    "    shape = engine.get_binding_shape(i)\n",
    "    if is_input:\n",
    "        batch_size = shape[0]\n",
    "    size = np.dtype(trt.nptype(dtype)).itemsize\n",
    "    for s in shape:\n",
    "        size *= s\n",
    "\n",
    "    allocation = cuda.mem_alloc(size)\n",
    "    \n",
    "    binding = {\n",
    "        'index': i,\n",
    "        'name': name,\n",
    "        'dtype': np.dtype(trt.nptype(dtype)),\n",
    "        'shape': list(shape),\n",
    "        'allocation': allocation,\n",
    "    }\n",
    "    \n",
    "    allocations.append(allocation)\n",
    "    if engine.binding_is_input(i):\n",
    "        inputs.append(binding)\n",
    "    else:\n",
    "        outputs.append(binding)\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)\n",
    "print(allocations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148e16f",
   "metadata": {},
   "source": [
    "#Step.3: Do a single image inference, to test the necessary packages have been imported correctly and the TensorRT model have been loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62ec98-e0be-42f6-ba49-d03e559dd944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Before we \"\"\"\n",
    "stream = cuda.Stream()\n",
    "# Prepare the output data\n",
    "output = np.zeros(outputs[0]['shape'], outputs[0]['dtype'])\n",
    "\n",
    "def asy_infer(batch,output):\n",
    "    \"\"\"\n",
    "    Execute inference on a batch of images. The images should already be batched and preprocessed, as prepared by\n",
    "    the ImageBatcher class. Memory copying to and from the GPU device will be performed here.\n",
    "    :param batch: A numpy array holding the image batch.\n",
    "    :param top: The number of classes to return as top_predicitons, in descending order by their score. By default,\n",
    "    setting to one will return the same as the maximum score class. Useful for Top-5 accuracy metrics in validation.\n",
    "    :return: Three items, as numpy arrays for each batch image: The maximum score class, the corresponding maximum\n",
    "    score, and a list of the top N classes and scores.\n",
    "    \"\"\"\n",
    "    # Process I/O and execute the network\n",
    "    cuda.memcpy_htod_async(inputs[0]['allocation'], np.ascontiguousarray(batch[0]),stream)\n",
    "    cuda.memcpy_htod_async(inputs[1]['allocation'], np.ascontiguousarray(batch[1]),stream)\n",
    "    \n",
    "    context.execute_async_v2(allocations,stream.handle,None)\n",
    "    \n",
    "    cuda.memcpy_dtoh_async(output, outputs[0]['allocation'],stream)\n",
    "    stream.synchronize()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0fb54-23df-4720-a607-c11148833905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create the validation dataloader\"\"\"\n",
    "val_data = Data.RGBD_Dataset(transform=torchvision.transforms.Compose([scaleNorm(),\n",
    "                                                                       ToTensor(),\n",
    "                                                                       Normalize()]),\n",
    "                             phase_train=False,\n",
    "                             data_dir='../NYUv2_numpy', # The file path of the NYUv2 dataset\n",
    "                             txt_name='test.txt'    # Data split. In evaluation, use the test.txt\n",
    "                             )\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d65859-58cb-4c17-909a-0b50417aa5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img=val_data[0]['image'].numpy().astype(np.float32)\n",
    "depth=val_data[0]['depth'].numpy().astype(np.float32)\n",
    "label=val_data[0]['label'].numpy().astype(np.float32)\n",
    "batch=[img,depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e16e2-567f-4cc3-841f-a3c42cd12189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Conduct a single image inference. \"\"\"\n",
    "torch.cuda.synchronize()\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\"\"\"Run the inference and record the time cost\n",
    "The inference speed will vary if you run this code block multiple times. \n",
    "You will observe that the initial run is slower compared to subsequent runs. \n",
    "This is because the GPU is in power-saving mode when it is not under any workload. \n",
    "Step 4 demonstrates the use of warm-up inference to prompt the GPU to exit power-saving mode.\n",
    "\"\"\"\n",
    "starter.record()\n",
    "out=asy_infer(batch,output)\n",
    "ender.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "curr_time = starter.elapsed_time(ender)\n",
    "print('Frame Per Second (FPS)ï¼š',1000/curr_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8f5fd-8da5-46cf-8a56-9e32c339d657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Show the visualization of the inference result\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "pred=out.argmax(axis=1)\n",
    "pred=pred.squeeze(axis=0)\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03e137",
   "metadata": {},
   "source": [
    "#Step.4: Inference speed test, without evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa8229-318b-4430-b3b3-151d4264f6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "acc_collect = []\n",
    "torch.cuda.synchronize()\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "timings = np.zeros((len(val_loader), 1))\n",
    "\n",
    "\"\"\"Caeate dummy inputs of rgb and depth, to run the warm-up\"\"\"\n",
    "img=np.random.rand(1,3,480,640).astype(np.float32)\n",
    "depth=np.random.rand(1,1,480,640).astype(np.float32)\n",
    "batch=[img,depth]\n",
    "\n",
    "with torch.no_grad():\n",
    "    \"\"\"Run a warm-up inference, giving the GPU a workload to exit power-saving mode\"\"\"\n",
    "    for _ in range(50):\n",
    "        _ = asy_infer(batch,output)\n",
    "\n",
    "    \"\"\"Run the inference speed test\"\"\"\n",
    "    for batch_idx, sample in enumerate(val_loader):\n",
    "\n",
    "        image = sample['image'].numpy().astype(np.float32)\n",
    "        depth = sample['depth'].numpy().astype(np.float32)\n",
    "        batch=[image,depth]\n",
    "\n",
    "        \"\"\"Run the inference and record the time cost\"\"\"\n",
    "        starter.record()\n",
    "        pred =asy_infer(batch,output)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[batch_idx] = curr_time\n",
    "\n",
    "\"\"\"Output the inference speed of AsymFormer\"\"\"\n",
    "print('Average Inference speed (ms): ', timings[2:].sum() / 653)\n",
    "print('Frame Per Second (FPS):',1000/(timings[2:].sum() / 653))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c35a1",
   "metadata": {},
   "source": [
    "#Step.5: Evaluation\n",
    "This code will output both inference speed and quantitative evaluation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62131f68-feac-4a26-a52e-c62fe07dd792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_meter = AverageMeter()\n",
    "intersection_meter = AverageMeter()\n",
    "union_meter = AverageMeter()\n",
    "a_meter = AverageMeter()\n",
    "b_meter = AverageMeter()\n",
    "t = 0\n",
    "acc_collect = []\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "timings = np.zeros((len(val_loader), 1))\n",
    "\n",
    "img=np.random.rand(1,3,480,640).astype(np.float32)\n",
    "depth=np.random.rand(1,1,480,640).astype(np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        _ = asy_infer(batch,output)\n",
    "\n",
    "    for batch_idx, sample in enumerate(val_loader):\n",
    "\n",
    "        image = sample['image'].numpy().astype(np.float32)\n",
    "        depth = sample['depth'].numpy().astype(np.float32)\n",
    "        label = sample['label'].numpy()\n",
    "        batch=[image,depth]\n",
    "        \n",
    "        starter.record()\n",
    "        pred =asy_infer(batch,output)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[batch_idx] = curr_time\n",
    "\n",
    "        out=pred.argmax(axis=1)+1\n",
    "        out=out.squeeze(axis=0)\n",
    "\n",
    "        acc, pix = accuracy(out, label)\n",
    "        acc_collect.append(acc)\n",
    "        intersection, union = intersectionAndUnion(out, label, 40)\n",
    "        acc_meter.update(acc, pix)\n",
    "        a_m, b_m = macc(out, label, 40)\n",
    "        intersection_meter.update(intersection)\n",
    "        union_meter.update(union)\n",
    "        a_meter.update(a_m)\n",
    "        b_meter.update(b_m)\n",
    "        print('[{}] iter {}, accuracy: {}'\n",
    "              .format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                      batch_idx, acc))\n",
    "\n",
    "\n",
    "iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "for i, _iou in enumerate(iou):\n",
    "    print('class [{}], IoU: {}'.format(i, _iou))\n",
    "\n",
    "mAcc = (a_meter.average() / (b_meter.average() + 1e-10))\n",
    "print(mAcc.mean())\n",
    "print('[Eval Summary]:')\n",
    "print('Mean IoU: {:.4}, Accuracy: {:.2f}%'\n",
    "      .format(iou.mean(), acc_meter.average() * 100))\n",
    "print('Average Inference speed (ms): ', timings[2:].sum() / 653)\n",
    "print('Frame Per Second (FPS):',1000/(timings[2:].sum() / 653))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4391579-8c25-42bd-8f49-73361bd48461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
